{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import platform\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mps_available() -> bool:\n",
    "    \"\"\"\n",
    "    Safely check if MPS (Metal Performance Shaders) is available.\n",
    "    \"\"\"\n",
    "    if platform.system() != \"Darwin\":  # MPS is only available on macOS\n",
    "        return False\n",
    "\n",
    "    # Check if the current PyTorch version has MPS support\n",
    "    if not hasattr(torch, \"backends\") or not hasattr(torch.backends, \"mps\"):\n",
    "        return False\n",
    "\n",
    "    return torch.backends.mps.is_available()\n",
    "\n",
    "\n",
    "def set_all_seeds(\n",
    "    seed: int, deterministic: bool = True, warn_only: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Set all seeds and deterministic flags for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to use for all random number generators\n",
    "        deterministic (bool): Whether to enforce deterministic behavior\n",
    "        warn_only (bool): If True, warning instead of error when deterministic\n",
    "                         operations aren't supported\n",
    "    \"\"\"\n",
    "    # Python RNG\n",
    "    random.seed(seed)\n",
    "\n",
    "    # NumPy RNG\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # PyTorch RNGs\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Handle CUDA devices\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not set CUDA seeds: {str(e)}\")\n",
    "\n",
    "    # Handle MPS device (Apple Silicon)\n",
    "    if is_mps_available():\n",
    "        try:\n",
    "            torch.mps.manual_seed(seed)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not set MPS seed: {str(e)}\")\n",
    "\n",
    "    # Environment variables\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    if deterministic:\n",
    "        try:\n",
    "            # CUDA deterministic behavior\n",
    "            if torch.cuda.is_available():\n",
    "                os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "                torch.backends.cudnn.deterministic = True\n",
    "                torch.backends.cudnn.benchmark = False\n",
    "\n",
    "            # Set deterministic algorithms\n",
    "            torch.use_deterministic_algorithms(True, warn_only=warn_only)\n",
    "\n",
    "        except Exception as e:\n",
    "            msg = f\"Warning: Could not enable deterministic mode. Error: {str(e)}\"\n",
    "            if not warn_only:\n",
    "                raise RuntimeError(msg)\n",
    "            print(msg)\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Get the most appropriate PyTorch device available.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The preferred device (CUDA > MPS > CPU)\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if is_mps_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_all_seeds(SEED, warn_only=True)\n",
    "device = get_device()\n",
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "model: HookedTransformer = HookedTransformer.from_pretrained(\n",
    "    model_name, device=device\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank = {\n",
    "    \"city-country\": {\n",
    "        \"bank\": {\n",
    "            \"Bangkok\": \"Thailand\",\n",
    "            \"Beijing\": \"China\",\n",
    "            \"Buenos Aires\": \"Argentina\",\n",
    "            \"Cape Town\": \"South Africa\",\n",
    "            \"Hong Kong\": \"China\",\n",
    "            \"Kuala Lumpur\": \"Malaysia\",\n",
    "            \"Los Angeles\": \"United States\",\n",
    "            \"Mexico City\": \"Mexico\",\n",
    "            \"New Delhi\": \"India\",\n",
    "            \"New York City\": \"United States\",\n",
    "            \"Paris\": \"France\",\n",
    "            \"Rio de Janeiro\": \"Brazil\",\n",
    "            \"Rome\": \"Italy\",\n",
    "            \"San Francisco\": \"United States\",\n",
    "            \"St. Petersburg\": \"Russia\",\n",
    "            \"Sydney\": \"Australia\",\n",
    "            \"Tokyo\": \"Japan\",\n",
    "            \"Toronto\": \"Canada\",\n",
    "        },\n",
    "        \"prompt\": \"%s is a city in the country of\",\n",
    "    },\n",
    "    \"city-continent\": {\n",
    "        \"bank\": {\n",
    "            \"Bangkok\": \"Asia\",\n",
    "            \"Beijing\": \"Asia\",\n",
    "            \"Buenos Aires\": \"South America\",\n",
    "            \"Cape Town\": \"Africa\",\n",
    "            \"Hong Kong\": \"Asia\",\n",
    "            \"Kuala Lumpur\": \"Asia\",\n",
    "            \"Los Angeles\": \"North America\",\n",
    "            \"Mexico City\": \"North America\",\n",
    "            \"New Delhi\": \"Asia\",\n",
    "            \"New York City\": \"North America\",\n",
    "            \"Paris\": \"Europe\",\n",
    "            \"Rio de Janeiro\": \"South America\",\n",
    "            \"Rome\": \"Europe\",\n",
    "            \"San Francisco\": \"North America\",\n",
    "            \"St. Petersburg\": \"Europe\",\n",
    "            \"Sydney\": \"Oceania\",\n",
    "            \"Tokyo\": \"Asia\",\n",
    "            \"Toronto\": \"North America\",\n",
    "        },\n",
    "        \"prompt\": \"%s is a city in the continent of\",\n",
    "    },\n",
    "    \"city-language\": {\n",
    "        \"bank\": {\n",
    "            \"Bangkok\": \"Thai\",\n",
    "            \"Beijing\": \"Chinese\",\n",
    "            \"Buenos Aires\": \"Spanish\",\n",
    "            \"Cape Town\": \"Afrikaans\",\n",
    "            \"Kuala Lumpur\": \"Malay\",\n",
    "            \"Los Angeles\": \"English\",\n",
    "            \"Mexico City\": \"Spanish\",\n",
    "            \"New Delhi\": \"Hindi\",\n",
    "            \"New York City\": \"English\",\n",
    "            \"Paris\": \"French\",\n",
    "            \"Rio de Janeiro\": \"Portuguese\",\n",
    "            \"Rome\": \"Italian\",\n",
    "            \"San Francisco\": \"English\",\n",
    "            \"St. Petersburg\": \"Russian\",\n",
    "            \"Sydney\": \"English\",\n",
    "            \"Tokyo\": \"Japanese\",\n",
    "            \"Toronto\": \"English\",\n",
    "        },\n",
    "        \"prompt\": \"%s is a city where the language spoken is\",\n",
    "    },\n",
    "    \"occupation-duty\": {\n",
    "        \"bank\": {\n",
    "            \"to plan and design the construction of buildings\": \"architect\",\n",
    "            \"to represent clients in court during trial\": \"lawyer\",\n",
    "            \"to diagnose and treat issues related to the teeth\": \"dentist\",\n",
    "            \"to create clothing and accessories according to current trends\": \"fashion designer\",\n",
    "            \"to report news and information\": \"journalist\",\n",
    "            \"to educate students in a classroom setting\": \"teacher\",\n",
    "            \"to capture images using cameras\": \"photographer\",\n",
    "            \"to lead and guide a team of representatives, employees, or workers\": \"manager\",\n",
    "            \"to code, test, and maintain computer programs and applications\": \"software developer\",\n",
    "            \"to prepare and cook food in a professional kitchen\": \"chef\",\n",
    "            \"to care for patients in a medical setting and assist doctors\": \"nurse\",\n",
    "            \"to enforce laws and protect citizens from crime\": \"police officer\",\n",
    "            \"to repair and maintain vehicles and machinery in a workshop, garage, or factory\": \"mechanic\",\n",
    "            \"to conduct research and experiments in a laboratory setting\": \"scientist\",\n",
    "            \"to create visual art using various mediums, such as paint, clay, or digital tools\": \"artist\",\n",
    "            \"to play musical instruments and perform for audiences in various settings\": \"musician\",\n",
    "        },\n",
    "        \"prompt\": \"My duties are %s; I am a\",\n",
    "    },\n",
    "    \"object-color\": {\n",
    "        \"bank\": {\n",
    "            \"apple\": \"red\",\n",
    "            \"banana\": \"yellow\",\n",
    "            \"carrot\": \"orange\",\n",
    "            \"grape\": \"purple\",\n",
    "            \"lemon\": \"yellow\",\n",
    "            \"lime\": \"green\",\n",
    "            \"orange\": \"orange\",\n",
    "            \"pear\": \"green\",\n",
    "            \"strawberry\": \"red\",\n",
    "            \"tomato\": \"red\",\n",
    "            \"blueberry\": \"blue\",\n",
    "            \"cherry\": \"red\",\n",
    "            \"eggplant\": \"purple\",\n",
    "            \"kiwi\": \"brown\",\n",
    "            \"peach\": \"orange\",\n",
    "            \"plum\": \"purple\",\n",
    "            \"watermelon\": \"green\",\n",
    "            \"avocado\": \"green\",\n",
    "        },\n",
    "        \"prompt\": \"The color of the %s is usually\",\n",
    "    },\n",
    "    \"object-size\": {\n",
    "        \"bank\": {\n",
    "            \"diamond\": \"millimeter\",\n",
    "            \"bamboo\": \"meter\",\n",
    "            \"axe\": \"meter\",\n",
    "            \"gopher\": \"centimeter\",\n",
    "            \"saffron\": \"millimeter\",\n",
    "            \"lime\": \"centimeter\",\n",
    "            \"turmeric\": \"centimeter\",\n",
    "            \"lion\": \"meter\",\n",
    "            \"violet\": \"centimeter\",\n",
    "            \"starfish\": \"centimeter\",\n",
    "            \"charcoal\": \"centimeter\",\n",
    "            \"turquoise\": \"centimeter\",\n",
    "            \"flamingo\": \"meter\",\n",
    "            \"pig\": \"meter\",\n",
    "            \"cornmeal\": \"centimeter\",\n",
    "            \"blackberry\": \"centimeter\",\n",
    "        },\n",
    "        \"prompt\": 'Considering the following units \" millimeter \", \" centimeter \", \" meter \", and \" kilometer \", the size of %s is commonly expressed in \"',\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(entity: str = \"city\", target: str = \"country\", num_prompts: int = 30):\n",
    "    pair = \"{entity}-{target}\".format(entity=entity, target=target)\n",
    "    data = bank[pair]\n",
    "    pairs = data[\"bank\"]\n",
    "    prompt = data[\"prompt\"]\n",
    "    cities = list(pairs.keys())\n",
    "\n",
    "    # Generate N 3-shot prompts: \"A is a city in the country of B. C is a city in the country of D. E is a city in the country of \" ground_truth = F\n",
    "    prompts = []\n",
    "    for i in range(num_prompts):\n",
    "        # Randomly sample 3 cities from the list + 1 ground truth city to be queried (all different)\n",
    "        cities_sampled = torch.randperm(len(cities))[:4]\n",
    "        countries_sampled = [pairs[cities[i]] for i in cities_sampled]\n",
    "        ground_truth = countries_sampled[-1]\n",
    "\n",
    "        # Generate the prompt\n",
    "        prompt_instance = (\n",
    "            prompt % cities[cities_sampled[0]]\n",
    "            + \" \"\n",
    "            + countries_sampled[0]\n",
    "            + \". \"\n",
    "            + prompt % cities[cities_sampled[1]]\n",
    "            + \" \"\n",
    "            + countries_sampled[1]\n",
    "            + \". \"\n",
    "            + prompt % cities[cities_sampled[2]]\n",
    "            + \" \"\n",
    "            + countries_sampled[2]\n",
    "            + \". \"\n",
    "            + prompt % cities[cities_sampled[3]]\n",
    "        )\n",
    "\n",
    "        prompts.append((prompt_instance, ground_truth))\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city - country\n",
    "city_country_prompts = make_prompt(entity=\"city\", target=\"country\", num_prompts=1000)\n",
    "\n",
    "# city - continent\n",
    "city_continent_prompts = make_prompt(entity=\"city\", target=\"continent\", num_prompts=1000)\n",
    "\n",
    "# city - language\n",
    "city_language_prompts = make_prompt(entity=\"city\", target=\"language\", num_prompts=1000)\n",
    "\n",
    "# duty - occupation\n",
    "duty_occupation_prompts = make_prompt(\n",
    "    entity=\"occupation\", target=\"duty\", num_prompts=1000\n",
    ")\n",
    "\n",
    "# object - color\n",
    "object_color_prompts = make_prompt(entity=\"object\", target=\"color\", num_prompts=1000)\n",
    "\n",
    "# object - size\n",
    "object_size_prompts = make_prompt(entity=\"object\", target=\"size\", num_prompts=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Buenos Aires is a city in the country of Argentina. Los Angeles is a city in the country of United States. New Delhi is a city in the country of India. Mexico City is a city in the country of', 'Mexico')\n",
      "('Cape Town is a city in the continent of Africa. Rome is a city in the continent of Europe. St. Petersburg is a city in the continent of Europe. Los Angeles is a city in the continent of', 'North America')\n",
      "('Buenos Aires is a city where the language spoken is Spanish. Los Angeles is a city where the language spoken is English. New York City is a city where the language spoken is English. St. Petersburg is a city where the language spoken is', 'Russian')\n",
      "('My duties are to lead and guide a team of representatives, employees, or workers; I am a manager. My duties are to repair and maintain vehicles and machinery in a workshop, garage, or factory; I am a mechanic. My duties are to conduct research and experiments in a laboratory setting; I am a scientist. My duties are to diagnose and treat issues related to the teeth; I am a', 'dentist')\n",
      "('The color of the banana is usually yellow. The color of the tomato is usually red. The color of the lime is usually green. The color of the pear is usually', 'green')\n",
      "('Considering the following units \" millimeter \", \" centimeter \", \" meter \", and \" kilometer \", the size of lion is commonly expressed in \" meter. Considering the following units \" millimeter \", \" centimeter \", \" meter \", and \" kilometer \", the size of flamingo is commonly expressed in \" meter. Considering the following units \" millimeter \", \" centimeter \", \" meter \", and \" kilometer \", the size of lime is commonly expressed in \" centimeter. Considering the following units \" millimeter \", \" centimeter \", \" meter \", and \" kilometer \", the size of turmeric is commonly expressed in \"', 'centimeter')\n"
     ]
    }
   ],
   "source": [
    "# Print some samples\n",
    "print(city_country_prompts[0])\n",
    "print(city_continent_prompts[0])\n",
    "print(city_language_prompts[0])\n",
    "print(duty_occupation_prompts[0])\n",
    "print(object_color_prompts[0])\n",
    "print(object_size_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.generate(\n",
    "#     \"She is living in Rome, therefore her country of residence is Italy. She is living in New Delhi, therefore her country of residence is India. She is living in Washington, therefore her country of residence is\",\n",
    "#     max_new_tokens=5,\n",
    "#     temperature=1.0,\n",
    "#     prepend_bos=True,\n",
    "# )  # This will print the output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_country_correct = 0\n",
    "city_country_retained = []\n",
    "for prompt, gt in city_country_prompts:\n",
    "    print(prompt)\n",
    "    generation = model.generate(\n",
    "        prompt, max_new_tokens=1, temperature=0.0, prepend_bos=True\n",
    "    )\n",
    "    print(generation)\n",
    "\n",
    "    print(\"Ground truth: \", gt)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    if gt.startswith(generation[len(prompt) :].strip()):\n",
    "        city_country_correct += 1\n",
    "        city_country_retained.append((prompt, gt))\n",
    "\n",
    "print(\"Accuracy: \", city_country_correct / len(city_country_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_continent_correct = 0\n",
    "\n",
    "\n",
    "city_continent_retained = []\n",
    "for prompt, gt in city_continent_prompts:\n",
    "    print(prompt)\n",
    "    generation = model.generate(\n",
    "        prompt, max_new_tokens=1, temperature=0.0, prepend_bos=True\n",
    "    )\n",
    "    print(generation)\n",
    "    print(\"Ground truth: \", gt)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    if gt.startswith(generation[len(prompt) :].strip()):\n",
    "        city_continent_correct += 1\n",
    "        city_continent_retained.append((prompt, gt))\n",
    "\n",
    "print(\"Accuracy: \", city_continent_correct / len(city_continent_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_language_correct = 0\n",
    "city_language_retained = []\n",
    "for prompt, gt in city_language_prompts:\n",
    "    print(prompt)\n",
    "    generation = model.generate(\n",
    "        prompt, max_new_tokens=1, temperature=0.0, prepend_bos=True\n",
    "    )\n",
    "    print(generation)\n",
    "    print(\"Ground truth: \", gt)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    if gt.startswith(generation[len(prompt) :].strip()):\n",
    "        city_language_correct += 1\n",
    "        city_language_retained.append((prompt, gt))\n",
    "\n",
    "print(\"Accuracy: \", city_language_correct / len(city_language_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duty_occupation_correct = 0\n",
    "duty_occupation_retained = []\n",
    "for prompt, gt in duty_occupation_prompts:\n",
    "    print(prompt)\n",
    "    generation = model.generate(\n",
    "        prompt, max_new_tokens=1, temperature=0.0, prepend_bos=True\n",
    "    )\n",
    "    print(generation)\n",
    "    print(\"Ground truth: \", gt)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    if gt.startswith(generation[len(prompt) :].strip()):\n",
    "        duty_occupation_correct += 1\n",
    "        duty_occupation_retained.append((prompt, gt))\n",
    "\n",
    "print(\"Accuracy: \", duty_occupation_correct / len(duty_occupation_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_color_correct = 0\n",
    "object_color_retained = []\n",
    "for prompt, gt in object_color_prompts:\n",
    "    print(prompt)\n",
    "    generation = model.generate(\n",
    "        prompt, max_new_tokens=1, temperature=0.0, prepend_bos=True\n",
    "    )\n",
    "    print(generation)\n",
    "    print(\"Ground truth: \", gt)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    if gt.startswith(generation[len(prompt) :].strip()):\n",
    "        object_color_correct += 1\n",
    "        object_color_retained.append((prompt, gt))\n",
    "\n",
    "print(\"Accuracy: \", object_color_correct / len(object_color_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_size_correct = 0\n",
    "object_size_retained = []\n",
    "for prompt, gt in object_size_prompts:\n",
    "    print(prompt)\n",
    "    generation = model.generate(\n",
    "        prompt, max_new_tokens=1, temperature=0.0, prepend_bos=True\n",
    "    )\n",
    "    print(generation)\n",
    "    print(\"Ground truth: \", gt)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    if gt.startswith(generation[len(prompt) :].strip()):\n",
    "        object_size_correct += 1\n",
    "        object_size_retained.append((prompt, gt))\n",
    "\n",
    "print(\"Accuracy: \", object_size_correct / len(object_size_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in a pickle file the data {(entity-target): {accuracy : accuracy, retained: [(prompt, gt)], full_data: [(prompt, gt)]}}\n",
    "\n",
    "data = {\n",
    "    \"city-country\": {\n",
    "        \"correct_samples_number\": city_country_correct,\n",
    "        \"accuracy\": city_country_correct / len(city_country_prompts),\n",
    "        \"retained\": city_country_retained,\n",
    "        \"full_data\": city_country_prompts,\n",
    "    },\n",
    "    \"city-continent\": {\n",
    "        \"correct_samples_number\": city_continent_correct,\n",
    "        \"accuracy\": city_continent_correct / len(city_continent_prompts),\n",
    "        \"retained\": city_continent_retained,\n",
    "        \"full_data\": city_continent_prompts,\n",
    "    },\n",
    "    \"city-language\": {\n",
    "        \"correct_samples_number\": city_language_correct,\n",
    "        \"accuracy\": city_language_correct / len(city_language_prompts),\n",
    "        \"retained\": city_language_retained,\n",
    "        \"full_data\": city_language_prompts,\n",
    "    },\n",
    "    \"occupation-duty\": {\n",
    "        \"correct_samples_number\": duty_occupation_correct,\n",
    "        \"accuracy\": duty_occupation_correct / len(duty_occupation_prompts),\n",
    "        \"retained\": duty_occupation_retained,\n",
    "        \"full_data\": duty_occupation_prompts,\n",
    "    },\n",
    "    \"object-color\": {\n",
    "        \"correct_samples_number\": object_color_correct,\n",
    "        \"accuracy\": object_color_correct / len(object_color_prompts),\n",
    "        \"retained\": object_color_retained,\n",
    "        \"full_data\": object_color_prompts,\n",
    "    },\n",
    "    \"object-size\": {\n",
    "        \"correct_samples_number\": object_size_correct,\n",
    "        \"accuracy\": object_size_correct / len(object_size_prompts),\n",
    "        \"retained\": object_size_retained,\n",
    "        \"full_data\": object_size_prompts,\n",
    "    },\n",
    "}\n",
    "\n",
    "os.makedirs(\"prompt_data/EleutherAI\", exist_ok=True)\n",
    "\n",
    "with open(f\"prompt_data/{model_name}_prompt_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for city-country: 329\n",
      "\n",
      "\n",
      "---------------------------------------------------\n",
      "Accuracy for city-continent: 318\n",
      "\n",
      "\n",
      "---------------------------------------------------\n",
      "Accuracy for city-language: 224\n",
      "\n",
      "\n",
      "---------------------------------------------------\n",
      "Accuracy for occupation-duty: 19\n",
      "\n",
      "\n",
      "---------------------------------------------------\n",
      "Accuracy for object-color: 108\n",
      "\n",
      "\n",
      "---------------------------------------------------\n",
      "Accuracy for object-size: 307\n",
      "\n",
      "\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Print a recap of the results\n",
    "for key, value in data.items():\n",
    "    print(f\"Accuracy for {key}: {value['correct_samples_number']}\")\n",
    "    print(\"\\n\")\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
