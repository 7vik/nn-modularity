{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import platform\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_mps_available() -> bool:\n",
    "    \"\"\"\n",
    "    Safely check if MPS (Metal Performance Shaders) is available.\n",
    "    \"\"\"\n",
    "    if platform.system() != \"Darwin\":  # MPS is only available on macOS\n",
    "        return False\n",
    "\n",
    "    # Check if the current PyTorch version has MPS support\n",
    "    if not hasattr(torch, \"backends\") or not hasattr(torch.backends, \"mps\"):\n",
    "        return False\n",
    "\n",
    "    return torch.backends.mps.is_available()\n",
    "\n",
    "\n",
    "def set_all_seeds(\n",
    "    seed: int, deterministic: bool = True, warn_only: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Set all seeds and deterministic flags for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to use for all random number generators\n",
    "        deterministic (bool): Whether to enforce deterministic behavior\n",
    "        warn_only (bool): If True, warning instead of error when deterministic\n",
    "                         operations aren't supported\n",
    "    \"\"\"\n",
    "    # Python RNG\n",
    "    random.seed(seed)\n",
    "\n",
    "    # NumPy RNG\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # PyTorch RNGs\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Handle CUDA devices\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not set CUDA seeds: {str(e)}\")\n",
    "\n",
    "    # Handle MPS device (Apple Silicon)\n",
    "    if is_mps_available():\n",
    "        try:\n",
    "            torch.mps.manual_seed(seed)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not set MPS seed: {str(e)}\")\n",
    "\n",
    "    # Environment variables\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    if deterministic:\n",
    "        try:\n",
    "            # CUDA deterministic behavior\n",
    "            if torch.cuda.is_available():\n",
    "                os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "                torch.backends.cudnn.deterministic = True\n",
    "                torch.backends.cudnn.benchmark = False\n",
    "\n",
    "            # Set deterministic algorithms\n",
    "            torch.use_deterministic_algorithms(True, warn_only=warn_only)\n",
    "\n",
    "        except Exception as e:\n",
    "            msg = f\"Warning: Could not enable deterministic mode. Error: {str(e)}\"\n",
    "            if not warn_only:\n",
    "                raise RuntimeError(msg)\n",
    "            print(msg)\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Get the most appropriate PyTorch device available.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The preferred device (CUDA > MPS > CPU)\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if is_mps_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "gpt12 not found. Valid official model names (excl aliases): ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'distilgpt2', 'facebook/opt-125m', 'facebook/opt-1.3b', 'facebook/opt-2.7b', 'facebook/opt-6.7b', 'facebook/opt-13b', 'facebook/opt-30b', 'facebook/opt-66b', 'EleutherAI/gpt-neo-125M', 'EleutherAI/gpt-neo-1.3B', 'EleutherAI/gpt-neo-2.7B', 'EleutherAI/gpt-j-6B', 'EleutherAI/gpt-neox-20b', 'stanford-crfm/alias-gpt2-small-x21', 'stanford-crfm/battlestar-gpt2-small-x49', 'stanford-crfm/caprica-gpt2-small-x81', 'stanford-crfm/darkmatter-gpt2-small-x343', 'stanford-crfm/expanse-gpt2-small-x777', 'stanford-crfm/arwen-gpt2-medium-x21', 'stanford-crfm/beren-gpt2-medium-x49', 'stanford-crfm/celebrimbor-gpt2-medium-x81', 'stanford-crfm/durin-gpt2-medium-x343', 'stanford-crfm/eowyn-gpt2-medium-x777', 'EleutherAI/pythia-14m', 'EleutherAI/pythia-31m', 'EleutherAI/pythia-70m', 'EleutherAI/pythia-160m', 'EleutherAI/pythia-410m', 'EleutherAI/pythia-1b', 'EleutherAI/pythia-1.4b', 'EleutherAI/pythia-2.8b', 'EleutherAI/pythia-6.9b', 'EleutherAI/pythia-12b', 'EleutherAI/pythia-70m-deduped', 'EleutherAI/pythia-160m-deduped', 'EleutherAI/pythia-410m-deduped', 'EleutherAI/pythia-1b-deduped', 'EleutherAI/pythia-1.4b-deduped', 'EleutherAI/pythia-2.8b-deduped', 'EleutherAI/pythia-6.9b-deduped', 'EleutherAI/pythia-12b-deduped', 'EleutherAI/pythia-70m-v0', 'EleutherAI/pythia-160m-v0', 'EleutherAI/pythia-410m-v0', 'EleutherAI/pythia-1b-v0', 'EleutherAI/pythia-1.4b-v0', 'EleutherAI/pythia-2.8b-v0', 'EleutherAI/pythia-6.9b-v0', 'EleutherAI/pythia-12b-v0', 'EleutherAI/pythia-70m-deduped-v0', 'EleutherAI/pythia-160m-deduped-v0', 'EleutherAI/pythia-410m-deduped-v0', 'EleutherAI/pythia-1b-deduped-v0', 'EleutherAI/pythia-1.4b-deduped-v0', 'EleutherAI/pythia-2.8b-deduped-v0', 'EleutherAI/pythia-6.9b-deduped-v0', 'EleutherAI/pythia-12b-deduped-v0', 'EleutherAI/pythia-160m-seed1', 'EleutherAI/pythia-160m-seed2', 'EleutherAI/pythia-160m-seed3', 'NeelNanda/SoLU_1L_v9_old', 'NeelNanda/SoLU_2L_v10_old', 'NeelNanda/SoLU_4L_v11_old', 'NeelNanda/SoLU_6L_v13_old', 'NeelNanda/SoLU_8L_v21_old', 'NeelNanda/SoLU_10L_v22_old', 'NeelNanda/SoLU_12L_v23_old', 'NeelNanda/SoLU_1L512W_C4_Code', 'NeelNanda/SoLU_2L512W_C4_Code', 'NeelNanda/SoLU_3L512W_C4_Code', 'NeelNanda/SoLU_4L512W_C4_Code', 'NeelNanda/SoLU_6L768W_C4_Code', 'NeelNanda/SoLU_8L1024W_C4_Code', 'NeelNanda/SoLU_10L1280W_C4_Code', 'NeelNanda/SoLU_12L1536W_C4_Code', 'NeelNanda/GELU_1L512W_C4_Code', 'NeelNanda/GELU_2L512W_C4_Code', 'NeelNanda/GELU_3L512W_C4_Code', 'NeelNanda/GELU_4L512W_C4_Code', 'NeelNanda/Attn_Only_1L512W_C4_Code', 'NeelNanda/Attn_Only_2L512W_C4_Code', 'NeelNanda/Attn_Only_3L512W_C4_Code', 'NeelNanda/Attn_Only_4L512W_C4_Code', 'NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr', 'NeelNanda/SoLU_1L512W_Wiki_Finetune', 'NeelNanda/SoLU_4L512W_Wiki_Finetune', 'ArthurConmy/redwood_attn_2l', 'llama-7b-hf', 'llama-13b-hf', 'llama-30b-hf', 'llama-65b-hf', 'meta-llama/Llama-2-7b-hf', 'meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Llama-2-13b-hf', 'meta-llama/Llama-2-13b-chat-hf', 'meta-llama/Llama-2-70b-chat-hf', 'codellama/CodeLlama-7b-hf', 'codellama/CodeLlama-7b-Python-hf', 'codellama/CodeLlama-7b-Instruct-hf', 'meta-llama/Meta-Llama-3-8B', 'meta-llama/Meta-Llama-3-8B-Instruct', 'meta-llama/Meta-Llama-3-70B', 'meta-llama/Meta-Llama-3-70B-Instruct', 'meta-llama/Llama-3.2-1B', 'meta-llama/Llama-3.2-3B', 'meta-llama/Llama-3.2-1B-Instruct', 'meta-llama/Llama-3.2-3B-Instruct', 'meta-llama/Llama-3.1-70B', 'meta-llama/Llama-3.1-8B', 'meta-llama/Llama-3.1-8B-Instruct', 'meta-llama/Llama-3.1-70B-Instruct', 'Baidicoot/Othello-GPT-Transformer-Lens', 'bert-base-cased', 'roneneldan/TinyStories-1M', 'roneneldan/TinyStories-3M', 'roneneldan/TinyStories-8M', 'roneneldan/TinyStories-28M', 'roneneldan/TinyStories-33M', 'roneneldan/TinyStories-Instruct-1M', 'roneneldan/TinyStories-Instruct-3M', 'roneneldan/TinyStories-Instruct-8M', 'roneneldan/TinyStories-Instruct-28M', 'roneneldan/TinyStories-Instruct-33M', 'roneneldan/TinyStories-1Layer-21M', 'roneneldan/TinyStories-2Layers-33M', 'roneneldan/TinyStories-Instuct-1Layer-21M', 'roneneldan/TinyStories-Instruct-2Layers-33M', 'stabilityai/stablelm-base-alpha-3b', 'stabilityai/stablelm-base-alpha-7b', 'stabilityai/stablelm-tuned-alpha-3b', 'stabilityai/stablelm-tuned-alpha-7b', 'mistralai/Mistral-7B-v0.1', 'mistralai/Mistral-7B-Instruct-v0.1', 'mistralai/Mistral-Nemo-Base-2407', 'mistralai/Mixtral-8x7B-v0.1', 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'bigscience/bloom-560m', 'bigscience/bloom-1b1', 'bigscience/bloom-1b7', 'bigscience/bloom-3b', 'bigscience/bloom-7b1', 'bigcode/santacoder', 'Qwen/Qwen-1_8B', 'Qwen/Qwen-7B', 'Qwen/Qwen-14B', 'Qwen/Qwen-1_8B-Chat', 'Qwen/Qwen-7B-Chat', 'Qwen/Qwen-14B-Chat', 'Qwen/Qwen1.5-0.5B', 'Qwen/Qwen1.5-0.5B-Chat', 'Qwen/Qwen1.5-1.8B', 'Qwen/Qwen1.5-1.8B-Chat', 'Qwen/Qwen1.5-4B', 'Qwen/Qwen1.5-4B-Chat', 'Qwen/Qwen1.5-7B', 'Qwen/Qwen1.5-7B-Chat', 'Qwen/Qwen1.5-14B', 'Qwen/Qwen1.5-14B-Chat', 'Qwen/Qwen2-0.5B', 'Qwen/Qwen2-0.5B-Instruct', 'Qwen/Qwen2-1.5B', 'Qwen/Qwen2-1.5B-Instruct', 'Qwen/Qwen2-7B', 'Qwen/Qwen2-7B-Instruct', 'Qwen/Qwen2.5-0.5B', 'Qwen/Qwen2.5-0.5B-Instruct', 'Qwen/Qwen2.5-1.5B', 'Qwen/Qwen2.5-1.5B-Instruct', 'Qwen/Qwen2.5-3B', 'Qwen/Qwen2.5-3B-Instruct', 'Qwen/Qwen2.5-7B', 'Qwen/Qwen2.5-7B-Instruct', 'Qwen/Qwen2.5-14B', 'Qwen/Qwen2.5-14B-Instruct', 'Qwen/Qwen2.5-32B', 'Qwen/Qwen2.5-32B-Instruct', 'Qwen/Qwen2.5-72B', 'Qwen/Qwen2.5-72B-Instruct', 'Qwen/QwQ-32B-Preview', 'microsoft/phi-1', 'microsoft/phi-1_5', 'microsoft/phi-2', 'microsoft/Phi-3-mini-4k-instruct', 'google/gemma-2b', 'google/gemma-7b', 'google/gemma-2b-it', 'google/gemma-7b-it', 'google/gemma-2-2b', 'google/gemma-2-2b-it', 'google/gemma-2-9b', 'google/gemma-2-9b-it', 'google/gemma-2-27b', 'google/gemma-2-27b-it', '01-ai/Yi-6B', '01-ai/Yi-34B', '01-ai/Yi-6B-Chat', '01-ai/Yi-34B-Chat', 'google-t5/t5-small', 'google-t5/t5-base', 'google-t5/t5-large', 'ai-forever/mGPT']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m get_device()\n\u001b[1;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt12\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model: HookedTransformer \u001b[38;5;241m=\u001b[39m \u001b[43mHookedTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:1274\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, first_n_layers, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat16 models may not work on CPU. Consider using a GPU or bfloat16.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;66;03m# Get the model name used in HuggingFace, rather than the alias.\u001b[39;00m\n\u001b[0;32m-> 1274\u001b[0m official_model_name \u001b[38;5;241m=\u001b[39m \u001b[43mloading\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_official_model_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;66;03m# Load the config into an HookedTransformerConfig object. If loading from a\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;66;03m# checkpoint, the config object will contain the information about the\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;66;03m# checkpoint\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m cfg \u001b[38;5;241m=\u001b[39m loading\u001b[38;5;241m.\u001b[39mget_pretrained_model_config(\n\u001b[1;32m   1280\u001b[0m     official_model_name,\n\u001b[1;32m   1281\u001b[0m     hf_cfg\u001b[38;5;241m=\u001b[39mhf_cfg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_pretrained_kwargs,\n\u001b[1;32m   1291\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/arena-env/lib/python3.11/site-packages/transformer_lens/loading_from_pretrained.py:723\u001b[0m, in \u001b[0;36mget_official_model_name\u001b[0;34m(model_name)\u001b[0m\n\u001b[1;32m    721\u001b[0m official_model_name \u001b[38;5;241m=\u001b[39m model_alias_map\u001b[38;5;241m.\u001b[39mget(model_name\u001b[38;5;241m.\u001b[39mlower(), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m official_model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    724\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found. Valid official model names (excl aliases): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOFFICIAL_MODEL_NAMES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    725\u001b[0m     )\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m official_model_name\n",
      "\u001b[0;31mValueError\u001b[0m: gpt12 not found. Valid official model names (excl aliases): ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'distilgpt2', 'facebook/opt-125m', 'facebook/opt-1.3b', 'facebook/opt-2.7b', 'facebook/opt-6.7b', 'facebook/opt-13b', 'facebook/opt-30b', 'facebook/opt-66b', 'EleutherAI/gpt-neo-125M', 'EleutherAI/gpt-neo-1.3B', 'EleutherAI/gpt-neo-2.7B', 'EleutherAI/gpt-j-6B', 'EleutherAI/gpt-neox-20b', 'stanford-crfm/alias-gpt2-small-x21', 'stanford-crfm/battlestar-gpt2-small-x49', 'stanford-crfm/caprica-gpt2-small-x81', 'stanford-crfm/darkmatter-gpt2-small-x343', 'stanford-crfm/expanse-gpt2-small-x777', 'stanford-crfm/arwen-gpt2-medium-x21', 'stanford-crfm/beren-gpt2-medium-x49', 'stanford-crfm/celebrimbor-gpt2-medium-x81', 'stanford-crfm/durin-gpt2-medium-x343', 'stanford-crfm/eowyn-gpt2-medium-x777', 'EleutherAI/pythia-14m', 'EleutherAI/pythia-31m', 'EleutherAI/pythia-70m', 'EleutherAI/pythia-160m', 'EleutherAI/pythia-410m', 'EleutherAI/pythia-1b', 'EleutherAI/pythia-1.4b', 'EleutherAI/pythia-2.8b', 'EleutherAI/pythia-6.9b', 'EleutherAI/pythia-12b', 'EleutherAI/pythia-70m-deduped', 'EleutherAI/pythia-160m-deduped', 'EleutherAI/pythia-410m-deduped', 'EleutherAI/pythia-1b-deduped', 'EleutherAI/pythia-1.4b-deduped', 'EleutherAI/pythia-2.8b-deduped', 'EleutherAI/pythia-6.9b-deduped', 'EleutherAI/pythia-12b-deduped', 'EleutherAI/pythia-70m-v0', 'EleutherAI/pythia-160m-v0', 'EleutherAI/pythia-410m-v0', 'EleutherAI/pythia-1b-v0', 'EleutherAI/pythia-1.4b-v0', 'EleutherAI/pythia-2.8b-v0', 'EleutherAI/pythia-6.9b-v0', 'EleutherAI/pythia-12b-v0', 'EleutherAI/pythia-70m-deduped-v0', 'EleutherAI/pythia-160m-deduped-v0', 'EleutherAI/pythia-410m-deduped-v0', 'EleutherAI/pythia-1b-deduped-v0', 'EleutherAI/pythia-1.4b-deduped-v0', 'EleutherAI/pythia-2.8b-deduped-v0', 'EleutherAI/pythia-6.9b-deduped-v0', 'EleutherAI/pythia-12b-deduped-v0', 'EleutherAI/pythia-160m-seed1', 'EleutherAI/pythia-160m-seed2', 'EleutherAI/pythia-160m-seed3', 'NeelNanda/SoLU_1L_v9_old', 'NeelNanda/SoLU_2L_v10_old', 'NeelNanda/SoLU_4L_v11_old', 'NeelNanda/SoLU_6L_v13_old', 'NeelNanda/SoLU_8L_v21_old', 'NeelNanda/SoLU_10L_v22_old', 'NeelNanda/SoLU_12L_v23_old', 'NeelNanda/SoLU_1L512W_C4_Code', 'NeelNanda/SoLU_2L512W_C4_Code', 'NeelNanda/SoLU_3L512W_C4_Code', 'NeelNanda/SoLU_4L512W_C4_Code', 'NeelNanda/SoLU_6L768W_C4_Code', 'NeelNanda/SoLU_8L1024W_C4_Code', 'NeelNanda/SoLU_10L1280W_C4_Code', 'NeelNanda/SoLU_12L1536W_C4_Code', 'NeelNanda/GELU_1L512W_C4_Code', 'NeelNanda/GELU_2L512W_C4_Code', 'NeelNanda/GELU_3L512W_C4_Code', 'NeelNanda/GELU_4L512W_C4_Code', 'NeelNanda/Attn_Only_1L512W_C4_Code', 'NeelNanda/Attn_Only_2L512W_C4_Code', 'NeelNanda/Attn_Only_3L512W_C4_Code', 'NeelNanda/Attn_Only_4L512W_C4_Code', 'NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr', 'NeelNanda/SoLU_1L512W_Wiki_Finetune', 'NeelNanda/SoLU_4L512W_Wiki_Finetune', 'ArthurConmy/redwood_attn_2l', 'llama-7b-hf', 'llama-13b-hf', 'llama-30b-hf', 'llama-65b-hf', 'meta-llama/Llama-2-7b-hf', 'meta-llama/Llama-2-7b-chat-hf', 'meta-llama/Llama-2-13b-hf', 'meta-llama/Llama-2-13b-chat-hf', 'meta-llama/Llama-2-70b-chat-hf', 'codellama/CodeLlama-7b-hf', 'codellama/CodeLlama-7b-Python-hf', 'codellama/CodeLlama-7b-Instruct-hf', 'meta-llama/Meta-Llama-3-8B', 'meta-llama/Meta-Llama-3-8B-Instruct', 'meta-llama/Meta-Llama-3-70B', 'meta-llama/Meta-Llama-3-70B-Instruct', 'meta-llama/Llama-3.2-1B', 'meta-llama/Llama-3.2-3B', 'meta-llama/Llama-3.2-1B-Instruct', 'meta-llama/Llama-3.2-3B-Instruct', 'meta-llama/Llama-3.1-70B', 'meta-llama/Llama-3.1-8B', 'meta-llama/Llama-3.1-8B-Instruct', 'meta-llama/Llama-3.1-70B-Instruct', 'Baidicoot/Othello-GPT-Transformer-Lens', 'bert-base-cased', 'roneneldan/TinyStories-1M', 'roneneldan/TinyStories-3M', 'roneneldan/TinyStories-8M', 'roneneldan/TinyStories-28M', 'roneneldan/TinyStories-33M', 'roneneldan/TinyStories-Instruct-1M', 'roneneldan/TinyStories-Instruct-3M', 'roneneldan/TinyStories-Instruct-8M', 'roneneldan/TinyStories-Instruct-28M', 'roneneldan/TinyStories-Instruct-33M', 'roneneldan/TinyStories-1Layer-21M', 'roneneldan/TinyStories-2Layers-33M', 'roneneldan/TinyStories-Instuct-1Layer-21M', 'roneneldan/TinyStories-Instruct-2Layers-33M', 'stabilityai/stablelm-base-alpha-3b', 'stabilityai/stablelm-base-alpha-7b', 'stabilityai/stablelm-tuned-alpha-3b', 'stabilityai/stablelm-tuned-alpha-7b', 'mistralai/Mistral-7B-v0.1', 'mistralai/Mistral-7B-Instruct-v0.1', 'mistralai/Mistral-Nemo-Base-2407', 'mistralai/Mixtral-8x7B-v0.1', 'mistralai/Mixtral-8x7B-Instruct-v0.1', 'bigscience/bloom-560m', 'bigscience/bloom-1b1', 'bigscience/bloom-1b7', 'bigscience/bloom-3b', 'bigscience/bloom-7b1', 'bigcode/santacoder', 'Qwen/Qwen-1_8B', 'Qwen/Qwen-7B', 'Qwen/Qwen-14B', 'Qwen/Qwen-1_8B-Chat', 'Qwen/Qwen-7B-Chat', 'Qwen/Qwen-14B-Chat', 'Qwen/Qwen1.5-0.5B', 'Qwen/Qwen1.5-0.5B-Chat', 'Qwen/Qwen1.5-1.8B', 'Qwen/Qwen1.5-1.8B-Chat', 'Qwen/Qwen1.5-4B', 'Qwen/Qwen1.5-4B-Chat', 'Qwen/Qwen1.5-7B', 'Qwen/Qwen1.5-7B-Chat', 'Qwen/Qwen1.5-14B', 'Qwen/Qwen1.5-14B-Chat', 'Qwen/Qwen2-0.5B', 'Qwen/Qwen2-0.5B-Instruct', 'Qwen/Qwen2-1.5B', 'Qwen/Qwen2-1.5B-Instruct', 'Qwen/Qwen2-7B', 'Qwen/Qwen2-7B-Instruct', 'Qwen/Qwen2.5-0.5B', 'Qwen/Qwen2.5-0.5B-Instruct', 'Qwen/Qwen2.5-1.5B', 'Qwen/Qwen2.5-1.5B-Instruct', 'Qwen/Qwen2.5-3B', 'Qwen/Qwen2.5-3B-Instruct', 'Qwen/Qwen2.5-7B', 'Qwen/Qwen2.5-7B-Instruct', 'Qwen/Qwen2.5-14B', 'Qwen/Qwen2.5-14B-Instruct', 'Qwen/Qwen2.5-32B', 'Qwen/Qwen2.5-32B-Instruct', 'Qwen/Qwen2.5-72B', 'Qwen/Qwen2.5-72B-Instruct', 'Qwen/QwQ-32B-Preview', 'microsoft/phi-1', 'microsoft/phi-1_5', 'microsoft/phi-2', 'microsoft/Phi-3-mini-4k-instruct', 'google/gemma-2b', 'google/gemma-7b', 'google/gemma-2b-it', 'google/gemma-7b-it', 'google/gemma-2-2b', 'google/gemma-2-2b-it', 'google/gemma-2-9b', 'google/gemma-2-9b-it', 'google/gemma-2-27b', 'google/gemma-2-27b-it', '01-ai/Yi-6B', '01-ai/Yi-34B', '01-ai/Yi-6B-Chat', '01-ai/Yi-34B-Chat', 'google-t5/t5-small', 'google-t5/t5-base', 'google-t5/t5-large', 'ai-forever/mGPT']"
     ]
    }
   ],
   "source": [
    "set_all_seeds(SEED, warn_only=True)\n",
    "device = get_device()\n",
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "model: HookedTransformer = HookedTransformer.from_pretrained(\n",
    "    model_name, device=device\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank = {\n",
    "    \"city-country\": {\n",
    "        \"bank\": {\n",
    "            \"Bangkok\": \"Thailand\",\n",
    "            \"Beijing\": \"China\",\n",
    "            \"Buenos Aires\": \"Argentina\",\n",
    "            \"Cape Town\": \"South Africa\",\n",
    "            \"Hong Kong\": \"China\",\n",
    "            \"Kuala Lumpur\": \"Malaysia\",\n",
    "            \"Los Angeles\": \"United States\",\n",
    "            \"Mexico City\": \"Mexico\",\n",
    "            \"New Delhi\": \"India\",\n",
    "            \"New York City\": \"United States\",\n",
    "            \"Paris\": \"France\",\n",
    "            \"Rio de Janeiro\": \"Brazil\",\n",
    "            \"Rome\": \"Italy\",\n",
    "            \"San Francisco\": \"United States\",\n",
    "            \"St. Petersburg\": \"Russia\",\n",
    "            \"Sydney\": \"Australia\",\n",
    "            \"Tokyo\": \"Japan\",\n",
    "            \"Toronto\": \"Canada\",\n",
    "        },\n",
    "        \"prompt\": \"%s is a city in the country of\",\n",
    "    },\n",
    "    \"city-continent\": {\n",
    "        \"bank\": {\n",
    "            \"Bangkok\": \"Asia\",\n",
    "            \"Beijing\": \"Asia\",\n",
    "            \"Buenos Aires\": \"South America\",\n",
    "            \"Cape Town\": \"Africa\",\n",
    "            \"Hong Kong\": \"Asia\",\n",
    "            \"Kuala Lumpur\": \"Asia\",\n",
    "            \"Los Angeles\": \"North America\",\n",
    "            \"Mexico City\": \"North America\",\n",
    "            \"New Delhi\": \"Asia\",\n",
    "            \"New York City\": \"North America\",\n",
    "            \"Paris\": \"Europe\",\n",
    "            \"Rio de Janeiro\": \"South America\",\n",
    "            \"Rome\": \"Europe\",\n",
    "            \"San Francisco\": \"North America\",\n",
    "            \"St. Petersburg\": \"Europe\",\n",
    "            \"Sydney\": \"Oceania\",\n",
    "            \"Tokyo\": \"Asia\",\n",
    "            \"Toronto\": \"North America\",\n",
    "        },\n",
    "        \"prompt\": \"%s is a city in the continent of\",\n",
    "    },\n",
    "    \"city-language\": {\n",
    "        \"bank\": {\n",
    "            \"Bangkok\": \"Thai\",\n",
    "            \"Beijing\": \"Chinese\",\n",
    "            \"Buenos Aires\": \"Spanish\",\n",
    "            \"Cape Town\": \"Afrikaans\",\n",
    "            \"Kuala Lumpur\": \"Malay\",\n",
    "            \"Los Angeles\": \"English\",\n",
    "            \"Mexico City\": \"Spanish\",\n",
    "            \"New Delhi\": \"Hindi\",\n",
    "            \"New York City\": \"English\",\n",
    "            \"Paris\": \"French\",\n",
    "            \"Rio de Janeiro\": \"Portuguese\",\n",
    "            \"Rome\": \"Italian\",\n",
    "            \"San Francisco\": \"English\",\n",
    "            \"St. Petersburg\": \"Russian\",\n",
    "            \"Sydney\": \"English\",\n",
    "            \"Tokyo\": \"Japanese\",\n",
    "            \"Toronto\": \"English\",\n",
    "        },\n",
    "        \"prompt\": \"%s is a city where the language spoken is\",\n",
    "    },\n",
    "    \"occupation-duty\": {\n",
    "        \"bank\": {\n",
    "            \"to plan and design the construction of buildings\": \"architect\",\n",
    "            \"to represent clients in court during trial\": \"lawyer\",\n",
    "            \"to diagnose and treat issues related to the teeth\": \"dentist\",\n",
    "            \"to create clothing and accessories according to current trends\": \"fashion designer\",\n",
    "            \"to report news and information\": \"journalist\",\n",
    "            \"to educate students in a classroom setting\": \"teacher\",\n",
    "            \"to capture images using cameras\": \"photographer\",\n",
    "            \"to lead and guide a team of representatives, employees, or workers\": \"manager\",\n",
    "            \"to code, test, and maintain computer programs and applications\": \"software developer\",\n",
    "            \"to prepare and cook food in a professional kitchen\": \"chef\",\n",
    "            \"to care for patients in a medical setting and assist doctors\": \"nurse\",\n",
    "            \"to enforce laws and protect citizens from crime\": \"police officer\",\n",
    "            \"to repair and maintain vehicles and machinery in a workshop, garage, or factory\": \"mechanic\",\n",
    "            \"to conduct research and experiments in a laboratory setting\": \"scientist\",\n",
    "            \"to create visual art using various mediums, such as paint, clay, or digital tools\": \"artist\",\n",
    "            \"to play musical instruments and perform for audiences in various settings\": \"musician\",\n",
    "        },\n",
    "        \"prompt\": \"My duties are %s; I am a\",\n",
    "    },\n",
    "    \"object-color\": {\n",
    "        \"bank\": {\n",
    "            \"apple\": \"red\",\n",
    "            \"banana\": \"yellow\",\n",
    "            \"carrot\": \"orange\",\n",
    "            \"grape\": \"purple\",\n",
    "            \"lemon\": \"yellow\",\n",
    "            \"lime\": \"green\",\n",
    "            \"orange\": \"orange\",\n",
    "            \"pear\": \"green\",\n",
    "            \"strawberry\": \"red\",\n",
    "            \"tomato\": \"red\",\n",
    "            \"blueberry\": \"blue\",\n",
    "            \"cherry\": \"red\",\n",
    "            \"eggplant\": \"purple\",\n",
    "            \"kiwi\": \"brown\",\n",
    "            \"peach\": \"orange\",\n",
    "            \"plum\": \"purple\",\n",
    "            \"watermelon\": \"green\",\n",
    "            \"avocado\": \"green\",\n",
    "        },\n",
    "        \"prompt\": \"The color of the %s is usually\",\n",
    "    },\n",
    "    \"object-size\": {\n",
    "        \"bank\": {\n",
    "            \"diamond\": \"millimeter\",\n",
    "            \"bamboo\": \"meter\",\n",
    "            \"axe\": \"meter\",\n",
    "            \"gopher\": \"centimeter\",\n",
    "            \"saffron\": \"millimeter\",\n",
    "            \"lime\": \"centimeter\",\n",
    "            \"turmeric\": \"centimeter\",\n",
    "            \"lion\": \"meter\",\n",
    "            \"violet\": \"centimeter\",\n",
    "            \"starfish\": \"centimeter\",\n",
    "            \"charcoal\": \"centimeter\",\n",
    "            \"turquoise\": \"centimeter\",\n",
    "            \"flamingo\": \"meter\",\n",
    "            \"pig\": \"meter\",\n",
    "            \"cornmeal\": \"centimeter\",\n",
    "            \"blackberry\": \"centimeter\",\n",
    "        },\n",
    "        \"prompt\": 'Considering the following units \" millimeter \", \" centimeter \", \" meter \", and \" kilometer \", the size of %s is commonly expressed in \"',\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(entity: str = \"city\", target: str = \"country\", num_prompts: int = 30):\n",
    "    pair = \"{entity}-{target}\".format(entity=entity, target=target)\n",
    "    data = bank[pair]\n",
    "    pairs = data[\"bank\"]\n",
    "    prompt = data[\"prompt\"]\n",
    "    cities = list(pairs.keys())\n",
    "\n",
    "    # Generate N 3-shot prompts: \"A is a city in the country of B. C is a city in the country of D. E is a city in the country of \" ground_truth = F\n",
    "    prompts = []\n",
    "    for i in range(num_prompts):\n",
    "        # Randomly sample 3 cities from the list + 1 ground truth city to be queried (all different)\n",
    "        cities_sampled = torch.randperm(len(cities))[:4]\n",
    "        countries_sampled = [pairs[cities[i]] for i in cities_sampled]\n",
    "        ground_truth = countries_sampled[-1]\n",
    "\n",
    "        # Generate the prompt\n",
    "        prompt_instance = (\n",
    "            prompt % cities[cities_sampled[0]]\n",
    "            + \" \"\n",
    "            + countries_sampled[0]\n",
    "            + \". \"\n",
    "            + prompt % cities[cities_sampled[1]]\n",
    "            + \" \"\n",
    "            + countries_sampled[1]\n",
    "            + \". \"\n",
    "            + prompt % cities[cities_sampled[2]]\n",
    "            + \" \"\n",
    "            + countries_sampled[2]\n",
    "            + \". \"\n",
    "            + prompt % cities[cities_sampled[3]]\n",
    "        )\n",
    "\n",
    "        prompts.append((prompt_instance, ground_truth))\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# city - country\n",
    "city_country_prompts = make_prompt(entity=\"city\", target=\"country\", num_prompts=100)\n",
    "\n",
    "# city - continent\n",
    "city_continent_prompts = make_prompt(entity=\"city\", target=\"continent\", num_prompts=100)\n",
    "\n",
    "# city - language\n",
    "city_language_prompts = make_prompt(entity=\"city\", target=\"language\", num_prompts=100)\n",
    "\n",
    "# duty - occupation\n",
    "duty_occupation_prompts = make_prompt(\n",
    "    entity=\"occupation\", target=\"duty\", num_prompts=100\n",
    ")\n",
    "\n",
    "# object - color\n",
    "object_color_prompts = make_prompt(entity=\"object\", target=\"color\", num_prompts=100)\n",
    "\n",
    "# object - size\n",
    "object_size_prompts = make_prompt(entity=\"object\", target=\"size\", num_prompts=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some samples\n",
    "print(city_country_prompts[0])\n",
    "print(city_continent_prompts[0])\n",
    "print(city_language_prompts[0])\n",
    "print(duty_occupation_prompts[0])\n",
    "print(object_color_prompts[0])\n",
    "print(object_size_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.generate(\n",
    "#     \"She is living in Rome, therefore her country of residence is Italy. She is living in New Delhi, therefore her country of residence is India. She is living in Washington, therefore her country of residence is\",\n",
    "#     max_new_tokens=5,\n",
    "#     temperature=1.0,\n",
    "#     prepend_bos=True,\n",
    "# )  # This will print the output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_country_correct = 0\n",
    "city_country_retained = []\n",
    "for prompt, gt in city_country_prompts:\n",
    "    print(prompt)\n",
    "    generation = model.generate(\n",
    "        prompt, max_new_tokens=1, temperature=0.0, prepend_bos=True\n",
    "    )\n",
    "    print(generation)\n",
    "\n",
    "    print(\"Ground truth: \", gt)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    if gt.startswith(generation[len(prompt) :].strip()):\n",
    "        city_country_correct += 1\n",
    "        city_country_retained.append((prompt, gt))\n",
    "\n",
    "print(\"Accuracy: \", city_country_correct / len(city_country_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_continent_correct = 0\n",
    "\n",
    "\n",
    "city_continent_retained = []\n",
    "for prompt, gt in city_continent_prompts:\n",
    "    print(prompt)\n",
    "    generation = model.generate(\n",
    "        prompt, max_new_tokens=1, temperature=0.0, prepend_bos=True\n",
    "    )\n",
    "    print(generation)\n",
    "    print(\"Ground truth: \", gt)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    if gt.startswith(generation[len(prompt) :].strip()):\n",
    "        city_continent_correct += 1\n",
    "        city_continent_retained.append((prompt, gt))\n",
    "\n",
    "print(\"Accuracy: \", city_continent_correct / len(city_continent_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_language_correct = 0\n",
    "city_language_retained = []\n",
    "for prompt, gt in city_language_prompts:\n",
    "    print(prompt)\n",
    "    generation = model.generate(\n",
    "        prompt, max_new_tokens=1, temperature=0.0, prepend_bos=True\n",
    "    )\n",
    "    print(generation)\n",
    "    print(\"Ground truth: \", gt)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    if gt.startswith(generation[len(prompt) :].strip()):\n",
    "        city_language_correct += 1\n",
    "        city_language_retained.append((prompt, gt))\n",
    "\n",
    "print(\"Accuracy: \", city_language_correct / len(city_language_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duty_occupation_correct = 0\n",
    "duty_occupation_retained = []\n",
    "for prompt, gt in duty_occupation_prompts:\n",
    "    print(prompt)\n",
    "    generation = model.generate(\n",
    "        prompt, max_new_tokens=1, temperature=0.0, prepend_bos=True\n",
    "    )\n",
    "    print(generation)\n",
    "    print(\"Ground truth: \", gt)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    if gt.startswith(generation[len(prompt) :].strip()):\n",
    "        duty_occupation_correct += 1\n",
    "        duty_occupation_retained.append((prompt, gt))\n",
    "\n",
    "print(\"Accuracy: \", duty_occupation_correct / len(duty_occupation_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_color_correct = 0\n",
    "object_color_retained = []\n",
    "for prompt, gt in object_color_prompts:\n",
    "    print(prompt)\n",
    "    generation = model.generate(\n",
    "        prompt, max_new_tokens=1, temperature=0.0, prepend_bos=True\n",
    "    )\n",
    "    print(generation)\n",
    "    print(\"Ground truth: \", gt)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    if gt.startswith(generation[len(prompt) :].strip()):\n",
    "        object_color_correct += 1\n",
    "        object_color_retained.append((prompt, gt))\n",
    "\n",
    "print(\"Accuracy: \", object_color_correct / len(object_color_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_size_correct = 0\n",
    "object_size_retained = []\n",
    "for prompt, gt in object_size_prompts:\n",
    "    print(prompt)\n",
    "    generation = model.generate(\n",
    "        prompt, max_new_tokens=1, temperature=0.0, prepend_bos=True\n",
    "    )\n",
    "    print(generation)\n",
    "    print(\"Ground truth: \", gt)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    if gt.startswith(generation[len(prompt) :].strip()):\n",
    "        object_size_correct += 1\n",
    "        object_size_retained.append((prompt, gt))\n",
    "\n",
    "print(\"Accuracy: \", object_size_correct / len(object_size_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in a pickle file the data {(entity-target): {accuracy : accuracy, retained: [(prompt, gt)], full_data: [(prompt, gt)]}}\n",
    "\n",
    "data = {\n",
    "    \"city-country\": {\n",
    "        \"accuracy\": city_country_correct / len(city_country_prompts),\n",
    "        \"retained\": city_country_retained,\n",
    "        \"full_data\": city_country_prompts,\n",
    "    },\n",
    "    \"city-continent\": {\n",
    "        \"accuracy\": city_continent_correct / len(city_continent_prompts),\n",
    "        \"retained\": city_continent_retained,\n",
    "        \"full_data\": city_continent_prompts,\n",
    "    },\n",
    "    \"city-language\": {\n",
    "        \"accuracy\": city_language_correct / len(city_language_prompts),\n",
    "        \"retained\": city_language_retained,\n",
    "        \"full_data\": city_language_prompts,\n",
    "    },\n",
    "    \"occupation-duty\": {\n",
    "        \"accuracy\": duty_occupation_correct / len(duty_occupation_prompts),\n",
    "        \"retained\": duty_occupation_retained,\n",
    "        \"full_data\": duty_occupation_prompts,\n",
    "    },\n",
    "    \"object-color\": {\n",
    "        \"accuracy\": object_color_correct / len(object_color_prompts),\n",
    "        \"retained\": object_color_retained,\n",
    "        \"full_data\": object_color_prompts,\n",
    "    },\n",
    "    \"object-size\": {\n",
    "        \"accuracy\": object_size_correct / len(object_size_prompts),\n",
    "        \"retained\": object_size_retained,\n",
    "        \"full_data\": object_size_prompts,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(f\"{model_name}_prompt_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a recap of the results\n",
    "for key, value in data.items():\n",
    "    print(f\"Accuracy for {key}: {value['accuracy']}\")\n",
    "    print(\"\\n\")\n",
    "    print(\"---------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arena-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
