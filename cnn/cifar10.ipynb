{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.colors as pc\n",
    "from plotly.subplots import make_subplots\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from itertools import islice\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import math\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "\n",
    "def randomseed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = 'CIFAR10' # 'MNIST' or 'CIFAR10'\n",
    "\n",
    "if dataset == 'MNIST':\n",
    "    transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    train_dataset = MNIST(root='.', train=True, download=True, transform=transform)\n",
    "    test_dataset = MNIST(root='.', train=False, download=True, transform=transform)\n",
    "elif dataset == 'CIFAR10':\n",
    "    transform = torchvision.transforms.ToTensor()\n",
    "    train_dataset = CIFAR10(root='.', train=True, download=True, transform=transform)\n",
    "    test_dataset = CIFAR10(root='.', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for MNIST\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 64, bias=False)\n",
    "        self.fc2 = nn.Linear(64, 64, bias=False)\n",
    "        self.fc3 = nn.Linear(64, 64, bias=False) # added\n",
    "        self.fc4 = nn.Linear(64, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "# for CIFAR10\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)  # Adjusted the input size here\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.bn1(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)  # Reduces dimensions by half (32x32 -> 16x16)\n",
    "        x = F.max_pool2d(x, 2)  # Reduces dimensions further (16x16 -> 8x8)\n",
    "        x = x.view(-1, 64 * 8 * 8)  # Flatten properly\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def new_model(dataset, device):\n",
    "    if dataset == 'MNIST':\n",
    "        model = MLP()\n",
    "    elif dataset == 'CIFAR10':\n",
    "        model = CNN()\n",
    "    model = model.to(device)\n",
    "    return model        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, data):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data:\n",
    "            outputs = model(images.to(device))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.to(device)).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "def classwise_accuracy(model, data):\n",
    "    model.eval()\n",
    "    correct = defaultdict(int)\n",
    "    total = defaultdict(int)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data:\n",
    "            outputs = model(images.to(device))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                total[label] += 1\n",
    "                correct[label] += int(predicted[i] == label)\n",
    "    return [round(correct[i] / total[i], 3) if total[i] > 0 else 0 for i in range(10)]\n",
    "\n",
    "def clusterability(matrix, auto_index=True, cluster_U_indices=None, cluster_V_indices=None, num_clusters=4):\n",
    "\n",
    "    if auto_index:\n",
    "        cluster_size = (matrix.shape[0] // num_clusters, matrix.shape[1] // num_clusters)\n",
    "        cluster_U_indices = {i: list(range(i*cluster_size[0], (i+1)*cluster_size[0])) for i in range(num_clusters)}\n",
    "        cluster_V_indices = {i: list(range(i*cluster_size[1], (i+1)*cluster_size[1])) for i in range(num_clusters)}\n",
    "\n",
    "    num_clusters = len(cluster_U_indices)\n",
    "    A = matrix ** 2\n",
    "    mask = torch.zeros_like(A, dtype=torch.bool)\n",
    "    \n",
    "    for cluster_idx in range(num_clusters):\n",
    "        u_indices = torch.tensor(cluster_U_indices[cluster_idx], dtype=torch.long)\n",
    "        v_indices = torch.tensor(cluster_V_indices[cluster_idx], dtype=torch.long)\n",
    "        mask[u_indices.unsqueeze(1), v_indices] = True\n",
    "    \n",
    "    intra_cluster_out_sum = torch.sum(A[mask])\n",
    "    total_out_sum = torch.sum(A)\n",
    "    \n",
    "    return intra_cluster_out_sum / total_out_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CIFAR10', device(type='cuda', index=0))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclustered_model = new_model(dataset, device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(unclustered_model.parameters(), lr=1e-3)\n",
    "train_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomseed(42)\n",
    "path = Path(f'checkpoints/{dataset}/')\n",
    "path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Accuracy: 0.1000\n",
      "Epoch 1/10, Train Loss: 1.0746, Accuracy: 0.4836, Clusterability: 0.9955\n",
      "Epoch 2/10, Train Loss: 1.3663, Accuracy: 0.6238, Clusterability: 0.9957\n",
      "Epoch 3/10, Train Loss: 1.1802, Accuracy: 0.6410, Clusterability: 0.9963\n",
      "Epoch 4/10, Train Loss: 1.2017, Accuracy: 0.6479, Clusterability: 0.9965\n",
      "Epoch 5/10, Train Loss: 1.0795, Accuracy: 0.6648, Clusterability: 0.9968\n",
      "Epoch 6/10, Train Loss: 0.8805, Accuracy: 0.6652, Clusterability: 0.9968\n",
      "Epoch 7/10, Train Loss: 0.7450, Accuracy: 0.6826, Clusterability: 0.9970\n",
      "Epoch 8/10, Train Loss: 0.5133, Accuracy: 0.7020, Clusterability: 0.9972\n",
      "Epoch 9/10, Train Loss: 0.7286, Accuracy: 0.7090, Clusterability: 0.9973\n",
      "Epoch 10/10, Train Loss: 1.4023, Accuracy: 0.7166, Clusterability: 0.9975\n"
     ]
    }
   ],
   "source": [
    "# print starting accuracy and loss\n",
    "acc = accuracy(unclustered_model, test_loader)\n",
    "print(f'Starting Accuracy: {acc:.4f}')\n",
    "\n",
    "for epoch in range(10):\n",
    "    unclustered_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = unclustered_model(data)\n",
    "        train_loss = criterion(output, target)\n",
    "        train_losses.append(train_loss.item())\n",
    "\n",
    "        ## CLUSTERABILITY\n",
    "\n",
    "        fc1_c = clusterability(unclustered_model.fc1.weight)\n",
    "        fc2_c = clusterability(unclustered_model.fc2.weight)\n",
    "        fc3_c = clusterability(unclustered_model.fc3.weight)\n",
    "\n",
    "        cluster_loss = (fc1_c + fc2_c + fc3_c) / 3\n",
    "\n",
    "        ## END CLUSTERABILITY\n",
    "\n",
    "        loss = train_loss - (20 * cluster_loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    acc = accuracy(unclustered_model, test_loader)\n",
    "    print(f'Epoch {epoch+1}/{10}, Train Loss: {train_loss.item():.4f}, Accuracy: {acc:.4f}, Clusterability: {cluster_loss.item():.4f}')\n",
    "    # save model\n",
    "torch.save(unclustered_model.state_dict(), path / 'fc_clustered_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1051161/4260030126.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unclustered_model.load_state_dict(torch.load(path / 'unclustered_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "unclustered_model.load_state_dict(torch.load(path / 'unclustered_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 4096]), torch.Size([64, 128]), torch.Size([10, 64]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unclustered_model.fc1.weight.shape, unclustered_model.fc2.weight.shape, unclustered_model.fc3.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2404, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(0.2502, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(0.2164, device='cuda:0', grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clusterability of unclustered model\n",
    "clusterability(unclustered_model.fc1.weight), clusterability(unclustered_model.fc2.weight), clusterability(unclustered_model.fc3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 4\n",
    "block = unclustered_model.fc1.weight\n",
    "cluster_size = (block.shape[0] // num_clusters, block.shape[1] // num_clusters)\n",
    "cluster_U_indices = {i: list(range(i*cluster_size[0], (i+1)*cluster_size[0])) for i in range(num_clusters)}\n",
    "cluster_V_indices = {i: list(range(i*cluster_size[1], (i+1)*cluster_size[1])) for i in range(num_clusters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2502, device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterability(block, cluster_U_indices, cluster_V_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "unclustered_model = new_model(dataset, device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(unclustered_model.parameters(), lr=1e-3)\n",
    "train_losses = []\n",
    "cluster_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Accuracy: 0.1000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m     acc \u001b[38;5;241m=\u001b[39m accuracy(unclustered_model, test_loader)\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtrain_losses\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Cluster Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster_losses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[1;32m     28\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(unclustered_model\u001b[38;5;241m.\u001b[39mstate_dict(), path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfc1_clustered_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "randomseed(42)\n",
    "path = Path(f'checkpoints/{dataset}/')\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "unclustered_model = new_model(dataset, device)\n",
    "\n",
    "# print starting accuracy and loss\n",
    "acc = accuracy(unclustered_model, test_loader)\n",
    "print(f'Starting Accuracy: {acc:.4f}')\n",
    "# cluster_loss = clusterability(unclustered_model.fc1.weight, cluster_U_indices, cluster_V_indices)\n",
    "# print(f'Starting Cluster Loss: {cluster_loss:.4f}')\n",
    "\n",
    "for epoch in range(10):\n",
    "    unclustered_model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = unclustered_model(data)\n",
    "        train_loss = criterion(output, target)\n",
    "        train_losses.append(train_loss.item())\n",
    "        cluster_loss = clusterability(unclustered_model.fc1.weight, cluster_U_indices, cluster_V_indices)\n",
    "        cluster_losses.append(cluster_loss.item())\n",
    "        loss = train_loss - (20 * cluster_loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    acc = accuracy(unclustered_model, test_loader)\n",
    "    print(f'Epoch {epoch+1}/{10}, Train Loss: {train_losses[-1].item():.4f}, Accuracy: {acc:.4f}, Cluster Loss: {cluster_losses[-1].item():.4f}')\n",
    "    # save model\n",
    "torch.save(unclustered_model.state_dict(), path / 'fc1_clustered_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
