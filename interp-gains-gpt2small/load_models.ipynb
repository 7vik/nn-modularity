{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Step 1: Download the two models from this link and store them in `./checkpoints`.\n",
    "\n",
    "Step 2: Use this notebook to load the models.\n",
    "\n",
    "Step 3: If needed, convert the models to a HuggingFace version. (this is TransformerLens and might have different variable names, but the model is functionally identical to gpt2-small.)\n",
    "\n",
    "Step 4: Interp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maheepchaudhary/miniforge3/envs/transit/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "import torch\n",
    "from plotly import graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maheepchaudhary/miniforge3/envs/transit/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_17436/3633571846.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  clustered_model.load_state_dict(torch.load(path_clustered, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clustered model\n",
    "\n",
    "clustered_model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "clustered_model.to(device)\n",
    "# path_clustered = '/home/b-sgolechha/research/nn-modularity/language-models/checkpoints/wiki_modular_mlp_in_model_epoch_6.pt'\n",
    "path_clustered = \"/Users/maheepchaudhary/pytorch/Projects/nn-modularity/interp-gains-gpt2small/data/models/new_wiki_modular_mlp_in_model_epoch_1.pt\"\n",
    "clustered_model.load_state_dict(torch.load(path_clustered, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maheepchaudhary/miniforge3/envs/transit/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/mkf1szd12yv28px5ztl_sjjr0000gn/T/ipykernel_17436/866031515.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  unclustered_model.load_state_dict(torch.load(path_unclustered, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unclustered model\n",
    "\n",
    "unclustered_model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "unclustered_model.to(device)\n",
    "# path_unclustered = '/home/b-sgolechha/research/nn-modularity/language-models/checkpoints/wiki_non_modular_mlp_in_model_epoch_2.pt'\n",
    "path_unclustered = \"/Users/maheepchaudhary/pytorch/Projects/nn-modularity/interp-gains-gpt2small/data/models/new_wiki_non_modular_mlp_in_model_epoch_1.pt\"\n",
    "unclustered_model.load_state_dict(torch.load(path_unclustered, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 20/20 [00:00<00:00, 35.13it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 29.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered model output:  The color of the darkness is <unk> and follows the east @-@ South direction of the country at the barrow,\n",
      "Unclustered model output:  The color of the darkness is up to 300 % compared to the other environments, although not always four @-@ shades considerably darker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# sanity check to see if the models are loaded correctly\n",
    "\n",
    "input = 'The color of the darkness is'\n",
    "\n",
    "clustered_output = clustered_model.generate(input, max_new_tokens=20)\n",
    "unclustered_output = unclustered_model.generate(input, max_new_tokens=20)\n",
    "\n",
    "print('Clustered model output: ', clustered_output)\n",
    "\n",
    "print('Unclustered model output: ', unclustered_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=10):   0%|          | 0/36718 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (3325 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3471 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=10):   3%|▎         | 1000/36718 [00:00<00:15, 2234.28 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (3546 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3670 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3652 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2941 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=10):  14%|█▎        | 5000/36718 [00:00<00:02, 11145.71 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (3641 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=10):  22%|██▏       | 8000/36718 [00:00<00:01, 15676.25 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (3384 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3828 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3869 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Map (num_proc=10): 100%|██████████| 36718/36718 [00:01<00:00, 26281.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# get loss and accuracy on wiki dataset\n",
    "\n",
    "from transformer_lens.evals import make_wiki_data_loader\n",
    "\n",
    "wiki = make_wiki_data_loader(unclustered_model.tokenizer, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maheepchaudhary/miniforge3/envs/transit/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50256,   796,   569,  ...,  1398,   764,  2080])\n",
      "<|endoftext|> = Valkyria Chronicles III = \n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3, lit. Valkyria of the Battlefield 3 ), commonly referred to as Valkyria Chronicles III outside Japan, is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable. Released in January 2011 in Japan, it is the third game in the Valkyria series. <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors, the story runs parallel to the first game and follows the \" Nameless \", a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \". \n",
      " The game began development in 2010, carrying over a large portion of the work done on Valkyria Chronicles II. While it retained the standard features of the series, it also underwent multiple adjustments, such as making the game more <unk> for series newcomers. Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries, along with Valkyria Chronicles II director Takeshi Ozawa. A large team of writers handled the script. The game's opening theme was sung by May 'n. \n",
      " It met with positive sales in Japan, and was praised by both Japanese and western critics. After release, it received downloadable content, along with an expanded edition in November of that year. It was also adapted into manga and an original video animation series. Due to low sales of Valkyria Chronicles II, Valkyria Chronicles III was not localized, but a fan translation compatible with the game's expanded edition was released in 2014. Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4. \n",
      " = = Gameplay = = \n",
      " As with previous <unk> Chronicles games, Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces. Stories are told through comic book @-@ like panels with animated character portraits, with characters speaking partially through voiced speech bubbles and partially through <unk> text. The player progresses through a series of linear missions, gradually unlocked as maps that can be freely <unk> through and replayed as they are unlocked. The route to each story location on the map varies depending on an individual player's approach : when one option is selected, the other is sealed off to the player. Outside missions, the player characters rest in a camp, where units can be customized and character growth occurs. Alongside the main story missions are character @-@ specific sub missions relating to different squad members. After the game's completion, additional episodes are unlocked, some of them having a higher difficulty than those found in the rest of the game. There are also love simulation elements related to the game's two main <unk>, although they take a very minor role. \n",
      " The game's battle system, the <unk> system, is carried over directly from <unk> Chronicles. During missions, players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected, the player moves the character around the battlefield in third @-@ person. A character can only act once per @-@ turn, but characters can be granted multiple turns at the expense of other characters'turns. Each character has a field and distance of movement limited by their Action <unk>. Up to nine characters can be assigned to a single mission. During gameplay, characters will call out if something happens to them, such as their health points ( HP ) getting low or being knocked out by enemy attacks. Each character has specific \" Potentials \", skills unique to each character. They are divided into \" Personal Potential \", which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character, and \" Battle Potentials \", which are grown throughout the game and always grant <unk> to a character. To learn Battle Potentials, each character has a unique \" Masters Table \", a grid @-@ based skill table that can be used to acquire and link different skills. Characters also have Special <unk> that grant them temporary <unk> on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without <unk> his Action Point gauge, the character <unk> can shift into her \" Valkyria Form \" and become <unk>, while Imca can target multiple enemy units with her heavy weapon. \n",
      " Troops are divided into five classes : Scouts, <unk>, Engineers, <unk> and Armored Soldier. <unk> can switch classes by changing their assigned weapon. Changing class does not greatly affect the stats gained while in a previous class. With\n",
      " a previous class. With\n",
      " class class. The the\n",
      "Unclustered accuracy: 49.169%\n",
      "Clustered accuracy: 44.379%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "for idx, batch in enumerate(wiki.dataset['tokens']):\n",
    "    \n",
    "    batch = batch.to(device)\n",
    "    print(batch)\n",
    "    print(tokenizer.decode(batch))\n",
    "    unclustered_logits = unclustered_model.forward(batch)\n",
    "    clustered_logits = clustered_model.forward(batch)\n",
    "    unclustered_predictions = torch.argmax(unclustered_model.forward(batch), dim=-1)\n",
    "    clustered_predictions = torch.argmax(clustered_model.forward(batch), dim=-1)\n",
    "\n",
    "    # accuracy on second last token\n",
    "    unclustered_accuracy = (unclustered_predictions[0, :-1] == batch[1:]).float().mean()\n",
    "    clustered_accuracy = (clustered_predictions[0, :-1] == batch[1:]).float().mean()\n",
    "    \n",
    "    print(tokenizer.decode(batch[-5:]))\n",
    "    print(tokenizer.decode(clustered_predictions[0, -5:]))\n",
    "\n",
    "    print(f'Unclustered accuracy: {round(float(100 * unclustered_accuracy), 3)}%')\n",
    "    print(f'Clustered accuracy: {round(float(100 * clustered_accuracy), 3)}%')\n",
    "\n",
    "    break\n",
    "\n",
    "# gt = <end of text> the ...... sbarn 1023\n",
    "# pred = the ..... sbarn, sdsdfsdf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enjoy interp!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
